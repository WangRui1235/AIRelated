(moshi) (base) wangrui@digital-life:~/shz$ python lab2.py
/home/wangrui/miniconda3/envs/moshi/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at model and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
改进前的test_data

训练集大小: 67349
列名: ['idx', 'sentence', 'label']
示例:
         idx                                           sentence  label
59113  59113          that the new film is a lame kiddie flick       0
56034  56034  reflect that its visual imagination is breatht...      1
35121  35121                 most multilayered and sympathetic       1

验证集大小: 872
列名: ['idx', 'sentence', 'label']
示例:
     idx                                           sentence  label
601  601                             fancy a real downer ?       0
844  844  given how heavy-handed and portent-heavy it is...      0
349  349  ... turns so unforgivably trite in its last 10...      0

测试集大小: 1821
列名: ['idx', 'sentence', 'label']
示例:
       idx                                           sentence  label
1525  1525  all the well-meaningness in the world ca n't e...     -1
1392  1392                              go see it and enjoy .     -1
867    867  though the controversial korean filmmaker 's l...     -1

Index(['idx', 'sentence', 'label'], dtype='object')
改进后的test_data

训练集大小: 53879
列名: ['idx', 'sentence', 'label']
示例:
         idx                                           sentence  label
23050  14928  his penchant for tearing up on cue -- things t...      1
41506  65216                                   expanded vision       1
5581   27335                          its own languorous charm       1

验证集大小: 872
列名: ['idx', 'sentence', 'label']
示例:
     idx                                           sentence  label
745  745  made with no discernible craft and monstrously...      0
172  172  it seems like i have been waiting my whole lif...      1
237  237  a by-the-numbers effort that wo n't do much to...      0

测试集大小: 13470
列名: ['idx', 'sentence', 'label']
示例:
         idx                                           sentence  label
12694  29282                             stunningly unoriginal       0
6969   13624            that embraces its old-fashioned themes       1
2108   61308  to speak about other than the fact that it is ...      0

/home/wangrui/miniconda3/envs/moshi/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Epoch 1/3: 100%|██████████████████████████████████████| 1684/1684 [02:08<00:00, 13.13it/s, training_loss=0.215, lr=2.22e-05]
Epoch 1/3, Loss: 0.2543
Epoch 1/3, Val Loss: 0.1994, Val Accuracy: 0.9186, Val Precision: 0.9187, Val Recall: 0.9186, Val F1: 0.9186
 best accuracy 0.9186)
Epoch 2/3: 100%|██████████████████████████████████████| 1684/1684 [02:08<00:00, 13.15it/s, training_loss=0.013, lr=1.11e-05]
Epoch 2/3, Loss: 0.1083
Epoch 2/3, Val Loss: 0.2219, Val Accuracy: 0.9255, Val Precision: 0.9255, Val Recall: 0.9255, Val F1: 0.9255
 best accuracy 0.9255)
Epoch 3/3: 100%|██████████████████████████████████████| 1684/1684 [02:08<00:00, 13.13it/s, training_loss=0.022, lr=0.00e+00]
Epoch 3/3, Loss: 0.0586
Epoch 3/3, Val Loss: 0.2416, Val Accuracy: 0.9232, Val Precision: 0.9233, Val Recall: 0.9232, Val F1: 0.9231
Testing: 100%|████████████████████████████████████████████████████████████████████████████| 421/421 [00:11<00:00, 37.98it/s]
随机输出5个错误案例:
Sentence: is just the point
True Label: 1, Predicted Label: 0

Sentence: sexy, violent, self - indulgent and maddening
True Label: 0, Predicted Label: 1

Sentence: you'd swear you
True Label: 1, Predicted Label: 0

Sentence: walks a tricky tightrope between being wickedly funny and just plain wicked
True Label: 0, Predicted Label: 1

Sentence: all but spits out denzel washington's fine performance in the title role.
True Label: 1, Predicted Label: 0


测试集结果:
准确率: 0.9532
精确率: 0.9534
召回率: 0.9532
F1分数: 0.9532


/home/wangrui/miniconda3/envs/moshi/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Some weights of the model checkpoint at model were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
                                                text
0  This is just a precious little diamond. The pl...
1  When I say this is my favourite film of all ti...
2  I saw this movie because I am a huge fan of th...
3  Being that the only foreign films I usually li...
4  After seeing Point of No Return (a great movie...
Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [01:21<00:00, 493.67 examples/s]
Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:20<00:00, 495.76 examples/s]
DatasetDict({
    train: Dataset({
        features: ['input_ids', 'token_type_ids', 'attention_mask'],
        num_rows: 40000
    })
    test: Dataset({
        features: ['input_ids', 'token_type_ids', 'attention_mask'],
        num_rows: 10000
    })
})
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 834/834 [08:04<00:00,  1.72it/s, pretraining_loss=2.130, lr=0.00e+00]
预训练完成!模型文件已保存在mlm_results


                                                text
0  I rented I AM CURIOUS-YELLOW from my video sto...
1  "I Am Curious: Yellow" is a risible and preten...
2  If only to avoid making this type of film in t...
3  This film was probably inspired by Godard's Ma...
4  Oh, brother...after hearing about this ridicul...
/home/wangrui/miniconda3/envs/moshi/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./mlm_results and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Epoch 1
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1407/1407 [01:50<00:00, 12.73it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 157/157 [00:06<00:00, 23.26it/s]
Train Loss: 0.3248
Val Accuracy: 0.8904, F1: 0.8904

Epoch 2
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1407/1407 [01:49<00:00, 12.84it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 157/157 [00:06<00:00, 23.28it/s]
Train Loss: 0.1861
Val Accuracy: 0.8932, F1: 0.8932

Epoch 3
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1407/1407 [01:49<00:00, 12.85it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 157/157 [00:06<00:00, 23.56it/s]
Train Loss: 0.0890
Val Accuracy: 0.8888, F1: 0.8888
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:06<00:00, 23.44it/s]

Final Test Performance:
Accuracy: 0.8886
F1 Score: 0.8886